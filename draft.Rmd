---
title: "Extra"
author: "by Valeria Castañeda Saucedo"
date: "2024-12-11"
output: html_document
---
```{r}
all_headlines_df <- data.frame(Page = integer(), Headline = character(), stringsAsFactors = FALSE)

# Loop through pages 1 to 20 (or more)
for (page_num in 1:400) {
  # Construct the URL for the current page
  WXYZ_url <- read_html(paste0("https://www.wxyz.com/search?q=food%20quality&p=", page_num))
  
  # Scrape the headline nodes
  WXYZ_nds <- html_nodes(WXYZ_url, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "ListItem-date", " " ))] | //*[contains(concat( " ", @class, " " ), concat( " ", "ListItem-title", " " ))]')
  
  # Extract the text from the headline nodes
  WXYZ_names <- html_text(WXYZ_nds)
  
  # Clean the headlines by trimming whitespace
  WXYZ_names_cleaned <- trimws(WXYZ_names)
  
  # Create a temporary data frame with the page number and headlines
  temp_df <- data.frame(Page = rep(page_num, length(WXYZ_names_cleaned)), 
                        Headline = WXYZ_names_cleaned, 
                        stringsAsFactors = FALSE)
  
  # Append the data from this page to the all_headlines_df data frame
  all_headlines_df <- rbind(all_headlines_df, temp_df)
}

head(all_headlines_df)
```


```{r}
all_headlines_df <- data.frame(Page = integer(), Headline = character(), stringsAsFactors = FALSE)

# Loop through pages 1 to 20 (or more)
for (page_num in 1:400) {
  # Construct the URL for the current page
  WXYZ_url <- read_html(paste0("https://www.wxyz.com/search?q=food%20quality&p=", page_num))
  
  # Scrape the headline nodes
  WXYZ_nds <- html_nodes(WXYZ_url, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "ListItem-title", " " ))]')
  
  # Extract the text from the headline nodes
  WXYZ_names <- html_text(WXYZ_nds)
  
  # Clean the headlines by trimming whitespace
  WXYZ_names_cleaned <- trimws(WXYZ_names)
  
  # Create a temporary data frame with the page number and headlines
  temp_df <- data.frame(Page = rep(page_num, length(WXYZ_names_cleaned)), 
                        Headline = WXYZ_names_cleaned, 
                        stringsAsFactors = FALSE)
  
  # Append the data from this page to the all_headlines_df data frame
  all_headlines_df <- rbind(all_headlines_df, temp_df)
}

head(all_headlines_df)
```
```{r}
# Initialize an empty dataframe to store the results
all_dates_df <- data.frame(Page = integer(),
                           DateTime = character(),
                           stringsAsFactors = FALSE)
# Loop through pages 1 to 400 (or as many pages as needed)
for (page_num in 1:400) {
  
  # Construct the URL for the current page
  WXYZ_url <- read_html(paste0("https://www.wxyz.com/search?q=food%20quality&p=", page_num))
  
  # Scrape the date/time nodes using the provided XPath
  WXYZ_dates <- html_nodes(WXYZ_url, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "ListItem-date", " " ))]')
  
  # Extract the text (date and time) from the date/time nodes
  WXYZ_dates_text <- html_text(WXYZ_dates)
  
  # Clean the extracted text by trimming whitespace
  WXYZ_dates_cleaned <- trimws(WXYZ_dates_text)
  
  # Create a temporary data frame with the page number and date/time
  temp_df <- data.frame(Page = rep(page_num, length(WXYZ_dates_cleaned)),
                        DateTime = WXYZ_dates_cleaned,
                        stringsAsFactors = FALSE)
  
  # Append the data from this page to the main dataframe
  all_dates_df <- rbind(all_dates_df, temp_df)
}

# Show the first few rows of the resulting dataframe
head(all_dates_df)
```



```{r}
# Start RSelenium with Chrome in headless mode
driver <- rsDriver(browser = "chrome",  
                   extraCapabilities = list(chromeOptions = list(args = list('--headless'))))

remDr <- driver[["client"]]

# URL of the NYT search page
url <- "https://www.nytimes.com/search?dropmab=false&endDate=2024-12-03&lang=en&query=food%20quality&sort=best&startDate=2020-01-01"

# Navigate to the page
remDr$navigate(url)

# Wait for the page to load initially
Sys.sleep(5)

# Function to extract data from the current page
extract_data <- function(remDr) {
  # Extract the headlines (by class name or XPath)
  headlines <- remDr$findElements(using = "xpath", value = "//*[contains(@class, 'css-16nhkrn')]")
  headlines_text <- sapply(headlines, function(x) x$getElementText())
  
  # Extract the snippets (or summaries) under each headline
  snippets <- remDr$findElements(using = "xpath", value = "//*[contains(@class, 'css-nsjm9t')]")
  snippets_text <- sapply(snippets, function(x) x$getElementText())
  
  # Create a data frame with the extracted data
  data <- data.frame(Headline = headlines_text, Snippet = snippets_text, stringsAsFactors = FALSE)
  return(data)
}

# Function to loop through the "Show More" button and scrape data
scrape_nyt_infinite_scroll <- function(remDr, num_results = 100) {
  all_data <- list()  # To store all extracted data
  total_results <- 0   # Keep track of the number of results scraped
  
  # Loop until we've scraped the desired number of results or the "Show More" button no longer exists
  while (total_results < num_results) {
    # Extract data from the current page
    page_data <- extract_data(remDr)
    all_data[[length(all_data) + 1]] <- page_data
    total_results <- total_results + nrow(page_data)  # Update the total number of results
    
    # If we've scraped enough results, break the loop
    if (total_results >= num_results) {
      break
    }
    
    # Try to click the "Show More" button (adjust the XPath if needed)
    try({
      show_more_button <- remDr$findElement(using = "xpath", value = "//button[contains(text(), 'Show more')]")
      show_more_button$clickElement()  # Click the button to load more results
      cat("Loaded more results. Total:", total_results, "\n")
    }, silent = TRUE)  # Ignore errors if the button is not found (no more results)
    
    # Wait for a few seconds to allow content to load before clicking again
    Sys.sleep(3)
  }
  
  # Combine all the data into one data frame
  result <- bind_rows(all_data)
  return(result)
}

# Scrape the first 100 results (you can adjust this number)
nyt_data <- scrape_nyt_infinite_scroll(remDr, num_results = 100)

# View the first few rows of the data
head(nyt_data)

# Save the scraped data to a CSV file
write.csv(nyt_data, "nyt_food_quality_articles.csv", row.names = FALSE)

```

```{r}
library(rvest)

# URL to scrape
url <- "https://www.nytimes.com/search?query=food+quality"

# Read the HTML content of the page
page <- read_html(url)

# Find article headings by class
headings <- page %>%
  html_nodes(".css-nsjm9t") %>%
  html_text()

# Print the headings
print(headings, 20)
```


```{r}
# Initialize an empty dataframe to store the results
all_headlines_df <- data.frame(Page = integer(),
                               Headline = character(),
                               DateTime = character(),
                               stringsAsFactors = FALSE)

# Loop through pages 1 to 400 (or as many pages as needed)
for (page_num in 1:100) {
  
  # Construct the URL for the current page
  WXYZ_url <- read_html(paste0("https://www.wxyz.com/search?q=food%20quality&p=", page_num))
  
  # Scrape both headline and date nodes using the XPath
  WXYZ_nds <- html_nodes(WXYZ_url, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "ListItem-date", " " ))] | //*[contains(concat( " ", @class, " " ), concat( " ", "ListItem-title", " " ))]')
  
  # Extract the text from the headline and date nodes
  WXYZ_text_cleaned  <- html_text(WXYZ_nds)
  
  # Clean the extracted text by trimming whitespace
  WXYZ_text_cleaned <- trimws(WXYZ_text_cleaned)
  
  # Extract headlines (the odd-indexed elements)
  WXYZ_headlines <- WXYZ_text_cleaned[seq(1, length(WXYZ_text_cleaned), by = 2)]
  
  # Extract dates (the even-indexed elements)
  WXYZ_dates <- WXYZ_text_cleaned[seq(2, length(WXYZ_text_cleaned), by = 2)]
  
  # Create a temporary data frame with page number, headlines, and date/time
  temp_df <- data.frame(Page = rep(page_num, length(WXYZ_headlines)),
                        Headline = WXYZ_headlines,
                        DateTime = WXYZ_dates,
                        stringsAsFactors = FALSE)
  
  # Append the data from this page to the all_headlines_df data frame
  all_headlines_df <- rbind(all_headlines_df, temp_df)
}

# Show the first few rows of the resulting dataframe
head(all_headlines_df)
```




```{r}
# URL for Honeycrisp Apples reviews page
apple_url <- "https://www.amazon.com/-/es/B000RGZMTQ-Manzana-Honeycrisp/dp/B000RGZMTQ/ref=cm_cr_arp_d_product_top?ie=UTF8#customerReviews"

# Set up the User-Agent header to mimic a real browser
user_agent <- "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"

# Send the GET request with the User-Agent header
apple_response <- GET(apple_url, add_headers("User-Agent" = user_agent))

# Parse the HTML content of the page
applepage_html <- content(apple_response, as = "text")
apple_page <- read_html(applepage_html)

# Scrape the reviews using the provided XPath
apple_reviews <- html_nodes(apple_page, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "a-text-bold", " " ))]//*[contains(concat( " ", @class, " " ), concat( " ", "cr-original-review-content", " " ))]')

# Extract the text from the review nodes
areviews_text <- html_text(apple_reviews)

# Clean the reviews text by removing leading/trailing spaces
areviews_cleaned <- trimws(areviews_text)

# Print the first few reviews
head(areviews_cleaned)
```




## Scarped brocolli food review from Amazon Fresh
```{r}
# URL for Broccoli (Amazon Fresh) reviews page
broccoli_url <- "https://www.amazon.com/-/es/Brócoli-orgánico-1-cabeza/dp/B08731CTJS/ref=sr_1_5_f3_wg?almBrandId=QW1hem9uIEZyZXNo&crid=3LRSTTEYWTRPE&dib=eyJ2IjoiMSJ9.hA0phk1Gz0C_itgI2D-LRxl5Svd3oXq4xk_pk3t4iFXq8jmwF7w51Bsa21QyT5JEV_7auGx--H1Wpyepzu9Na3-9qR3TCsGyLXGIJgyM9CyHnfizgiPs46yJzMYsOOjyPZO4N5yYdGOzlgc6dBYAQtinRzB65tfMEdhltEkF_6FvbKOVVxJNFUqarvDTeDIO173SLhTRD61_8C8l-2nEjWGHAWN712zGY0lQoATJ-CQPIWcV4u47F4Za6crpCjO-h7JLXb3cLxzbVn_YyUaUxRlIPXdSlLGf5AGGVBwq-uo.UqUIW0iT7SOjaKc9YiJak-bwWz5EayzB9eaBSxMzu0E&dib_tag=se&fpw=alm&keywords=broccoli&qid=1733261779&s=amazonfresh&sprefix=broc%2Camazonfresh%2C101&sr=1-5#customerReviews"

# Set up the User-Agent header to mimic a real browser
user_agent <- "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"

# Send the GET request with the User-Agent header
broccoli_response <- GET(broccoli_url, add_headers("User-Agent" = user_agent))

# Parse the HTML content of the page
broccoli_html <- content(broccoli_response, as = "text")
broccoli_page <- read_html(broccoli_html)

# Scrape the reviews using the provided XPath
broccoli_reviews <- html_nodes(broccoli_page, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "a-text-bold", " " ))]//*[contains(concat( " ", @class, " " ), concat( " ", "cr-original-review-content", " " ))]')

# Extract the text from the review nodes
broccoli_reviews_text <- html_text(broccoli_reviews)

# Clean the reviews text by removing leading/trailing spaces
broccoli_reviews_cleaned <- trimws(broccoli_reviews_text)

# Print the first few reviews
head(broccoli_reviews_cleaned, 5)
```



## Scarped rice food review from Amazon Fresh
```{r}
# URL for Mahatma Organic Brown Rice reviews page
rice_url <- "https://www.amazon.com/-/es/904351SU-Mahatma-Arroz-integral-orgánico/dp/B000JOQ57E/ref=sr_1_5_f3_wg?__mk_es_US=ÅMÅŽÕÑ&almBrandId=QW1hem9uIEZyZXNo&crid=1FZG50UHXXCYA&dib=eyJ2IjoiMSJ9.IeoIkqOsup1GRgSd6S5UmcD2lI6T0yN5famSAV6hNIgw-L4IrwtaxjyHaegsGKvDqEe4n8i8Lv-J1jY7E2zFnd7vCRzBoPXzRvZ7D4x_WV4k1vOE037vtvpyMU2k-CdJLMJNUEbHjXCDyDmjZdCRP1gWq2DjLlocYUHnXLR2-_r7OdQciC03DCbtC0Vqqt0CYif_X-9JLbaCdQP6G0icc8B0lcsk-uK3aZ5-T7KwdKGWCd5JtU9dehcZTMOazzMsdOcR-ac5Erlqydbiv7Gpthc9aWcnLNDs_LBXSxn2m5H7WXG0aMq8J7-s__iAEOwrn9TUQ7UBAMNLv69HogiJJEqSrhB4xzO4Xdxn7Qxnb-w.zx42NEDafChhqZzI7kkb1djyIxu3FzahAUPkhgQ4R2Q&dib_tag=se&fpw=alm&keywords=rice&qid=1733885644&s=amazonfresh&sprefix=rice%2Camazonfresh%2C170&sr=1-5&th=1"

# Set up the User-Agent header to mimic a real browser
user_agent <- "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"

# Send the GET request with the User-Agent header
rice_response <- GET(rice_url, add_headers("User-Agent" = user_agent))

# Parse the HTML content of the page
rice_html <- content(rice_response, as = "text")
rice_page <- read_html(rice_html)

# Scrape the reviews using the provided XPath
rice_reviews <- html_nodes(rice_page, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "a-text-bold", " " ))]//*[contains(concat( " ", @class, " " ), concat( " ", "cr-original-review-content", " " ))]')

# Extract the text from the review nodes
rice_reviews_text <- html_text(rice_reviews)

# Clean the reviews text by removing leading/trailing spaces
rice_reviews_cleaned <- trimws(rice_reviews_text)

# Print the first few reviews
head(rice_reviews_cleaned)
```


```{r}
# URL of the Amazon page
egg_url <- "https://www.amazon.com/-/es/Marca-Amazon-blancos-grandes-unidades/dp/B07ZS7B3VM/ref=sr_1_1_f3_wg?__mk_es_US=ÅMÅŽÕÑ&almBrandId=QW1hem9uIEZyZXNo&crid=1VJ8LGWK9O3TW&dib=eyJ2IjoiMSJ9.3dUmNAqyJi1aqE4v9N--5dsFobRPqn2vOcfEMeFHou7IBB_yW_jCmn1YUYq41cwNo4YSN9D6tcSvGdfunCubBbPaULjynGSVI1ojUR04Rg2-57HSg_ViVVaQvqjOa0Zoym5xi0O3T9UgpCTO_fMycCR38L2jvIyagHZSRHgR176Bo88IRc-msDOuKfN_HMra6xaF2ZbdP77V2xwzrEVMsD_fXO_ZldipQLsZ3jgeVK9NBn9dyOhcbPMV6WxveHl8NGE9DAlBz1OQLO0lrY2I6qC6nb39n8N5eNawlt0Zz9A.-IIWVEe27EzqMKouXylsUCSFBrrYl3I-x4s4NBYpQpo&dib_tag=se&fpw=alm&keywords=eggs&qid=1733261892&s=amazonfresh&sprefix=eggs%2Camazonfresh%2C119&sr=1-1"

# Parse the HTML content of the page
egg_page_html <- content(egg_response, as = "text")
egg_page <- read_html(egg_page_html)

# Scrape the review titles using the provided XPath
egg_review_titles <- html_nodes(egg_page, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "a-text-bold", " " ))]//*[contains(concat( " ", @class, " " ), concat( " ", "cr-original-review-content", " " ))]')

# Extract the text of the review titles
egg_review_titles_text <- html_text(egg_review_titles)

# Clean the review titles text by trimming whitespace
egg_review_titles_cleaned <- trimws(egg_review_titles_text)

# Print the first few cleaned review titles
head(egg_review_titles_cleaned)
```

